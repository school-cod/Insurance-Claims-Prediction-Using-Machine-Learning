{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c8ded9-015a-4e0e-9468-92b46b3b6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('claims_data.csv')\n",
    "\n",
    "# Convert target variable to binary\n",
    "df['insurance_claim'] = (df['insurance_claim'] == 'yes').astype(int)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(['insurance_claim', 'claim_amount'], axis=1)\n",
    "y = df['insurance_claim']\n",
    "\n",
    "# Split the data with stratify to maintain class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['sex', 'smoker', 'region']\n",
    "numeric_features = ['age', 'bmi', 'steps', 'children']\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25774aca-8443-44d4-a44f-af8bc4d3e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.358454\n",
      "         Iterations 8\n",
      "Accuracy: 0.8778\n",
      "Confusion Matrix:\n",
      "[[152  31]\n",
      " [ 23 236]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       183\n",
      "           1       0.88      0.91      0.90       259\n",
      "\n",
      "    accuracy                           0.88       442\n",
      "   macro avg       0.88      0.87      0.87       442\n",
      "weighted avg       0.88      0.88      0.88       442\n",
      "\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        insurance_claim   No. Observations:                  896\n",
      "Model:                          Logit   Df Residuals:                      886\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sat, 14 Sep 2024   Pseudo R-squ.:                  0.4718\n",
      "Time:                        10:21:02   Log-Likelihood:                -321.17\n",
      "converged:                       True   LL-Null:                       -608.10\n",
      "Covariance Type:            nonrobust   LLR p-value:                8.510e-118\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0119      0.215      0.055      0.956      -0.409       0.433\n",
      "x1             0.4336      0.108      4.018      0.000       0.222       0.645\n",
      "x2             1.9382      0.209      9.284      0.000       1.529       2.347\n",
      "x3             0.2524      0.153      1.651      0.099      -0.047       0.552\n",
      "x4            -1.8394      0.148    -12.461      0.000      -2.129      -1.550\n",
      "x5             0.3237      0.198      1.632      0.103      -0.065       0.712\n",
      "x6             4.4954      0.471      9.547      0.000       3.573       5.418\n",
      "x7            -0.3602      0.274     -1.315      0.188      -0.897       0.177\n",
      "x8            -0.3914      0.285     -1.373      0.170      -0.950       0.167\n",
      "x9            -0.2012      0.282     -0.715      0.475      -0.753       0.351\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Add a constant to the transformed data\n",
    "X_train_transformed = sm.add_constant(X_train_transformed)\n",
    "X_test_transformed = sm.add_constant(X_test_transformed)\n",
    "\n",
    "# Fit the logistic regression model using statsmodels\n",
    "model = sm.Logit(y_train, X_train_transformed).fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Convert predictions to binary values (0 or 1)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_test, y_pred_binary)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc973c5d-79cb-4449-9ecd-06f44328b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m X_train_encoded, X_test_encoded, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      7\u001b[0m     X_encoded, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create a random forest classifier with 100 trees and a random seed of 101\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m rf_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m101\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Fit the random forest model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m rf_classifier\u001b[38;5;241m.\u001b[39mfit(X_train_encoded, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding on the categorical variables\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Split features and target\n",
    "X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create a random forest classifier with 100 trees and a random seed of 101\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "\n",
    "# Fit the random forest model\n",
    "rf_classifier.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_encoded)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optional: Print feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b6a4b-bdf9-4d24-add9-95b0843c171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Perform one-hot encoding on the categorical variables\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "X_encoded = encoder.fit_transform(X[categorical_features])\n",
    "\n",
    "# Combine the numeric features with the encoded categorical features\n",
    "X_combined = np.hstack([\n",
    "    StandardScaler().fit_transform(X[numeric_features]),\n",
    "    X_encoded\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create a random forest classifier with 100 trees and a random seed of 101\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "\n",
    "# Fit the random forest model\n",
    "rf_classifier.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test_encoded)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Optional: Print feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Get feature names for encoded variables\n",
    "cat_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "feature_names = numeric_features + cat_feature_names.tolist()\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12928f-723d-4b9a-83c4-51c96730524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract false negatives and false positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
